# -*- coding: utf-8 -*-
"""Sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10PJN8_Xuh3rXREVenusEL15Ykk52rRpk
"""

#import tarfile
#!wget "https://github.com/aaronkub/machine-learning-examples/blob/master/imdb-sentiment-analysis/movie_data.tar.gz"

!ls
!echo
#!unzip
!file movie_data.tar.gz
!tar xvf movie_data.tar.gz

from google.colab import files
uploaded = files.upload()

!tar xvf 'movie_data.tar.gz'

!ls movie_data
reviews_train = []
for line in open('movie_data/full_train.txt', 'r'):
    reviews_train.append(line.strip())

reviews_test = []
for line in open('movie_data/full_test.txt', 'r'):
    reviews_test.append(line.strip())

import re

REPLACE_NO_SPACE = re.compile("[.;:!\'?,\"()\[\]]")
REPLACE_WITH_SPACE = re.compile("(<br\s*/><br\s*/>)|(\-)|(\/)")

def preprocess_reviews(reviews):
    reviews = [REPLACE_NO_SPACE.sub("", line.lower()) for line in reviews]
    reviews = [REPLACE_WITH_SPACE.sub(" ", line) for line in reviews]

    return reviews

reviews_train_clean = preprocess_reviews(reviews_train)
reviews_test_clean = preprocess_reviews(reviews_test)

print(reviews_train[0])
print(reviews_train_clean[14000])

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(binary=True)
cv.fit(reviews_train_clean)
X = cv.transform(reviews_train_clean)
X_test = cv.transform(reviews_test_clean)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

target = [1 if i < 12500 else 0 for i in range(25000)]

X_train, X_val, y_train, y_val = train_test_split(
    X, target, train_size = 0.75
)

for c in [0.01, 0.05, 0.25, 0.5, 1]:

    lr = LogisticRegression(C=c)
    lr.fit(X_train, y_train)
    print ("Accuracy for C=%s: %s"
#            % (c, accuracy_score(y_val, lr.predict(X_val))))

# Commented out IPython magic to ensure Python compatibility.
final_model = LogisticRegression(C=0.05)
final_model.fit(X, target)
print ("Final Accuracy: %s"
#        % accuracy_score(target, final_model.predict(X_test)))

feature_to_coef = {
    word: coef for word, coef in zip(
        cv.get_feature_names(), final_model.coef_[0]
    )
}
for best_positive in sorted(
    feature_to_coef.items(),
    key=lambda x: x[1],
    reverse=True)[:5]:
    print (best_positive)



for best_negative in sorted(
    feature_to_coef.items(),
    key=lambda x: x[1])[:5]:
    print (best_negative)

#print ("Final Accuracy: %s", final_model.predict(X_test[0]))
#print(X_test[0])
#for key, value in feature_to_coef.items():
  #print("key: ", key, " value: ", value)
#print(feature_to_coef.items())

print(feature_to_coef.get('amazing'))

maxVal = feature_to_coef.get(max(feature_to_coef, key=feature_to_coef.get))
minVal = feature_to_coef.get(min(feature_to_coef, key=feature_to_coef.get))
difference = maxVal-minVal
print((maxVal+abs(minVal))/difference)

def normalize(valRange, minVal, positivity):
  return (positivity+abs(minVal))/valRange

#print(normalize(difference, minVal, minVal))
wordPositivity = {}
for key, value in feature_to_coef.items():
  print(key, value)
  wordPositivity[key] = normalize(difference, minVal, value)
#print(wordPositivity['perfect'])

#print(reviews_train_clean[0])
def calculatePositivityScore(wordPositivity, review):
  total = sum = 0
  for word in review.split():
    if word in wordPositivity:
      total += 1
      sum += wordPositivity[word]
      print(word, wordPositivity[word])
  print("Positivity Score: ", sum/total)

def calculatePositivityScore2(wordPositivity, review):
  total = sum = 0
  for word in review.split():
    if word in wordPositivity:
      positivity = wordPositivity[word]
      if positivity > 0.7 or positivity < 0.4:
        total += 1
        sum += (wordPositivity[word] - 0.095)
        print(word, wordPositivity[word])
  #print("Positivity Score 2: ", sum/total )

#calculatePositivityScore(wordPositivity, reviews_train_clean[24780])
calculatePositivityScore2(wordPositivity, reviews_train_clean[61])

import matplotlib.pyplot as plt

numBins = int(len(wordPositivity)/1000)
for key, value in wordPositivity.items():
  wordPositivity[key] -= 0.095
print(numBins)
x = wordPositivity.values()
plt.hist(x, bins = numBins)
plt.show()

def filterWords(words):
  filtered = list(words)
  print(filtered)
  for word in filtered:
    if word > 0.65 or word < 0.51:
      filtered.remove(word)
    word
  return filtered

x = filterWords(wordPositivity.values())
plt.hist(x, bins = numBins, range=(0.4, 0.651))
plt.show()

'''
for sentence in range(0, len(features)):
    # Remove all the special characters
    processed_feature = re.sub(r'\W', ' ', str(features[sentence]))

    # remove all single characters
    processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

    # Remove single characters from the start
    processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature)

    # Substituting multiple spaces with single space
    processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

    # Removing prefixed 'b'
    processed_feature = re.sub(r'^b\s+', '', processed_feature)

    # Converting to Lowercase
    processed_feature = processed_feature.lower()

    processed_features.append(processed_feature)
'''

#from nltk.corpus import stopwords

english_stop_words = ['if', 'but', 'we', 'he', 'she', 'and', 'they', 'the']
def remove_stop_words(corpus):
    removed_stop_words = []
    for review in corpus:
        removed_stop_words.append(
            ' '.join([word for word in review.split()
                      if word not in english_stop_words])
        )
    return removed_stop_words

no_stop_words = remove_stop_words(reviews_train_clean)

# Commented out IPython magic to ensure Python compatibility.
cv = CountVectorizer(binary=True, ngram_range=(1, 3))
cv.fit(reviews_train_clean)
X = cv.transform(reviews_train_clean)
X_test = cv.transform(reviews_test_clean)


target = [1 if i < 12500 else 0 for i in range(25000)]

X_train, X_val, y_train, y_val = train_test_split(
    X, target, train_size = 0.75
)

for c in [0.01, 0.05, 0.25, 0.5, 1]:

    lr = LogisticRegression(C=c)
    lr.fit(X_train, y_train)
    print ("Accuracy for C=%s: %s"
#            % (c, accuracy_score(y_val, lr.predict(X_val))))


final_model = LogisticRegression(C=0.05)
final_model.fit(X, target)
print ("Final Accuracy: %s"
#        % accuracy_score(target, final_model.predict(X_test)))
